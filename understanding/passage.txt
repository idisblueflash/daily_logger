Backpropagation
Gradient Descent Solution
We adjust the old x pushing it in the direction of gradx with the force learning_rate.
Subtracting learning_rate * gradx.
Remember the gradient is initially in the direction of steepest ascent so subtracting learning_rate * gradx from x turns it into steepest descent. 
You can make sure of this yourself by replacing the subtraction with an addition.
The Gradient & Backpropagation
Now that we know how to update our weights and biases using the gradient, we need to figure out how to calculate the gradients for all of our nodes. 
For each node, we'll want to change the values based on the gradient of the cost with respect to the value of that node. 
In this way, the gradient descent updates we make will eventually converge to the minimum of the cost.
Let's consider a network with a linear node l1​​, a sigmoid node s, and another linear node l2, followed by an MSE node to calculate the cost, C.
Forward pass for a simple two layer network.
Writing this out in MiniFlow, it would look like:
We can see that each of the values of these nodes flows forwards and eventually produces the cost C. 
For example, the value of the second linear node l​2 goes into the cost node and determines the value of that node
 Accordingly, a change in l2 will produce a change in C. 
We can write this relationship between the changes as a gradient,
This is what a gradient means, it's a slope, how much you change the cost ∂C given a change in l2, ∂l2. 
So a node with a larger gradient with respect to the cost is going to contribute a larger change to the cost. 
In this way, we can assign blame for the cost to each node. 
The larger the gradient for a node, the more blame it gets for the final cost. 
And the more blame a node has, the more we'll update it in the gradient descent step.
 we want to update one of the weights with gradient descent, we'll need the gradient of the cost with respect to those weights. 
Let's see how we can use this framework to find the gradient for the weights in the second layer, w2. 
We want to calculate the gradient of C with respect to w2:
We can see in the graph that w2 is connected to l2, so a change in w2 is going to create a change in l2 which then creates a change in C. 
We can assign blame to w2 by sending the cost gradient back through the network. 
First you have how much l2 affected C, then how much w2 affected l2. 
Multiplying these gradients together gets you the total blame attributed to w2.
Pre-requisites
Below we're getting into the math behind backpropagation which requires multivariable calculus. 
If you need a refresher, I highly recommend checking out
Khan Academy's lessons on partial derivatives
Another video on gradients
And finally, using the chain rule
Continuing on
Multiplying these gradients is just an application of the chain rule:
